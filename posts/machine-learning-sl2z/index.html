<!doctype html><html><head><meta name=google-site-verification content="NqeNq4Fv5GSCvmbSVpNiquNu_jSZzfjMUc-eTSHC47w"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Can a machine learn mathematical structure? - pramana's blog</title><link rel=icon type=image/png href=/favicon.png>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A discussion of my research work last semester to use machine learning to answer questions in algebra"><meta property="og:image" content><meta property="og:url" content="https://pr4-kp.github.io/posts/machine-learning-sl2z/"><meta property="og:site_name" content="pramana's blog"><meta property="og:title" content="Can a machine learn mathematical structure?"><meta property="og:description" content="A discussion of my research work last semester to use machine learning to answer questions in algebra"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-23T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Algebra"><meta name=twitter:card content="summary"><meta name=twitter:title content="Can a machine learn mathematical structure?"><meta name=twitter:description content="A discussion of my research work last semester to use machine learning to answer questions in algebra"><script src=/js/feather.min.js></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href=https://pr4-kp.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200;300;400;500;600;700;800;900&display=swap" rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://pr4-kp.github.io/css/main.7e7a607d1958753408c98b811b65162967c330695e2b39d2b1f3e8dd78db2d0d.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://pr4-kp.github.io/css/dark.98977dfc7dfbbe146f608ac9600d4cc012fcf7d6dfd6991b932550e70ac25d99.css disabled></head><script>feather.replace()</script><body><div class=content><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><header><div class=main><a href=https://pr4-kp.github.io/>pramana's blog</a></div><nav><a href=/posts>all posts</a>
<a href=/notes>notes</a>
<a href=/tags>tags</a>
<a href=/about>about</a>
| <span id=dark-mode-toggle onclick=toggleTheme()></span>
<script src=https://pr4-kp.github.io//js/themetoggle.js></script></nav></header><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><main><article><div class=title><h1 class=title>Can a machine learn mathematical structure?</h1><div class=meta>Posted on Jul 23, 2024</div></div><section class=body><p>When I think of machine learning, I tend to think about <strong>estimation</strong>; a machine learning algorithm is a method of estimating a function given a (usually large) set of data. As a result, I&rsquo;ve primarily seen machine learning used when the function we are trying to estimate is assumed to be continuous (or differentiable, smooth, etc.), because then you can use nice results from analysis to prove convergence. But algebraic structures like <a href=https://en.wikipedia.org/wiki/Group_theory><strong>groups</strong></a> can be discrete objects, where there is no way to smoothly interpolate between two group elements. Moreover, groups adhere to very strict rules that dictate how they can be studied, which makes this continuous way of thinking hard to apply &ndash; I mean, it&rsquo;s hard to imagine that a machine learning model, just armed with data about groups, functions between them, and some backpropagation, could reach the <a href=https://en.wikipedia.org/wiki/Isomorphism_theorems>isomorphism theorems</a>. Despite this, the research I did last semester opened my mind more to using classical machine learning methods to get structure of discrete groups, given that we ask train the machine on the right questions.</p><p>I would like to thank our faculty advisor, <a href=https://people.math.wisc.edu/~ellenberg/>Professor Jordan Ellenberg</a>, our graduate student advisor, <a href=https://karansrivastava.com/>Karan Srivastava</a>, and my fellow undergraduates Sophia Cohen, Donald Conway, Noah Jillson, Jimmy Vineyard, and Alex Yun, for their hard work on this project.</p><p>All of our code for this project is available on <a href=https://github.com/MXM-MachineLearning/MXM_AI_Ellenberg>GitHub</a>.</p><h2 id=background-on-modular-forms-and-our-problem>Background on modular forms and our problem</h2><p>A popular topic in mathematics is the study of <a href=https://en.wikipedia.org/wiki/Modular_form><strong>modular forms</strong></a> (especially in the 1900s, culminating with the proof of <a href=https://en.wikipedia.org/wiki/Modularity_theorem>Fermat&rsquo;s last theorem</a>).<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> The basic idea with modular forms is to let the group $\mathrm{SL}_2(\Z)$ (the set of $2\times 2$ matrices with integer entries and determinant $1$), otherwise known as the <strong>modular group</strong>, interact with the upper-half plane $\mathbb{H}$ of the complex numbers by the equation</p><p>$$
\begin{bmatrix}
a & b \\ c & d
\end{bmatrix} \cdot z \coloneqq \frac{az+b}{cz+d},
$$</p><p>and study how it changes the upper-half plane.
It can be shown that this is a <a href=https://en.wikipedia.org/wiki/Group_action><strong>group action</strong></a> on $\mathbb{H}$, so it has a quotient $\mathbb{H}/\mathrm{SL}_2(\Z)$ (i.e. identifying two points if there is a matrix that can get us between them using the action above) that is <a href=https://en.wikipedia.org/wiki/Fundamental_domain#Fundamental_domain_for_the_modular_group>very interesting</a>. Most applications of this action rely on studying &ldquo;nice&rdquo; complex-valued functions that are unchanged (up to some fudging factor) by the group action.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> We call these <strong>modular forms</strong>.</p><p>Another question we may ask is what happens if we look at modular forms that are unchanged under the action of some <em>subgroup</em> $H \leq \mathrm{SL}_2(\Z)$? For instance, Jacobi proved in 1834 an <a href=https://en.wikipedia.org/wiki/Jacobi%27s_four-square_theorem#Theorem>exact formula</a> for the number of $4$-tuples of integers $(a,b,c,d)$ that satisfy $a^2+b^2+c^2+d^2=k$ for each integer $k$. A common proof uses modular forms, where the group $H$ is</p><p>$$
\Gamma_0(4) \coloneqq \left\{
\begin{bmatrix}
a & b \\ c & d
\end{bmatrix} \in \mathrm{SL}_2(\Z) \mid \begin{bmatrix}
a & b \\ c & d
\end{bmatrix} \equiv \begin{bmatrix}
* & * \\ 0 & *
\end{bmatrix} \pmod{4}
\right\},
$$</p><p>i.e. matrices in $\mathrm{SL}_2(\Z)$ whose entries reduced $\bmod{\:4}$ have a zero in the bottom-left.</p><h3 id=finite-index-subgroups-of-the-modular-group>Finite-index subgroups of the modular group</h3><p>So subgroups of the modular group are useful. Can we classify all of them? Since $\mathrm{SL}_2(\Z)$ is infinite, it would be a lot of work. In the context of modular forms, the finite subgroups are of little importance. In fact, some infinite subgroups are not useful. For a subgroup to be &ldquo;useful,&rdquo; we want the size of it to be not too much smaller than the size of $\mathrm{SL}_2(\Z)$. But then both of the groups would infinite! However, with some careful definitions in group theory, we can define what it means for some subgroup to contain some fraction of the elements of $\mathrm{SL}_2(\Z)$.</p><blockquote class=Definition><p><b>Definition</b><b> (Subgroup index)</b><b>.</b>
If $\mathrm{SL}_2(\Z)$ contains $n$ copies of $H$ inside it, then we say that $H$ is <a href=https://en.wikipedia.org/wiki/Index_of_a_subgroup><strong>index</strong></a> $n$ in $\mathrm{SL}_2(\Z)$.</p></blockquote><p>It is particularly important to us to find the <em>finite index</em> subgroups of $\mathrm{SL}_2(\Z)$. One example is the $\Gamma_0(N)$ group defined above (where we replace $4$ with $N$ for any natural number). Another example (and the main star of the show) is the <strong>Sanov subgroup</strong></p><p>$$
\left\langle \begin{bmatrix}
1 & 2 \\ 0 & 1
\end{bmatrix}, \begin{bmatrix}
1 & 0 \\ 2 & 1
\end{bmatrix}\right\rangle,
$$</p><p>which is the group of all matrices that are products of $\begin{bmatrix}
1 & 2 \\ 0 & 1
\end{bmatrix}$, $\begin{bmatrix}
1 & 0 \\ 2 & 1
\end{bmatrix}$, and their inverses.</p><p>Our goal was to make a computer learn how to determine if a subgroup of the modular group, or more generally $\mathrm{SL}_n(\Z)$ for $n\geq 3$, had finite or infinite index. This problem does not have an easy solution in general, and some questions are still unanswered for $n=3$.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><h2 id=reframing-the-problem-for-a-computer>Reframing the problem for a computer</h2><p>For here until the end $H\leq \mathrm{SL}_2(\Z)$ will be the Sanov subgroup defined before. A helpful visualization of copies of a subgroup sitting inside the bigger group is the following:</p><figure><img src=/img/machine-learning-sl2z/sl2z.png></figure><p>Here, there are finitely many copies of $H$ sitting inside $\mathrm{SL}_2(\Z)$. I will now start calling these copies by their proper name in group theory, <strong>cosets</strong>, and we will denote them $H,g_1H,\dots,g_5H$. Let&rsquo;s look at the coset $H$. Since each element of $H$ is a product of the matrices $\begin{bmatrix}
1 & 2 \\ 0 & 1
\end{bmatrix}$, $\begin{bmatrix}
1 & 0 \\ 2 & 1
\end{bmatrix}$, and their inverses (which we will call <strong>generators</strong>), we can imagine some path from any matrix, $A$, to the identity matrix $I=\begin{bmatrix}
1 & 0 \\ 0 & 1
\end{bmatrix}$ created by multiplying these generators.</p><figure><img src=/img/machine-learning-sl2z/generator-path.png></figure><p>It is a fact from group theory that none of these paths cross into the other cosets (for example, a path in $H$ doesn&rsquo;t cross into $g_1H$). Therefore, if we can find a path from any matrix to its respective &ldquo;identity matrix&rdquo; in any of the cosets, we can determine what coset it belongs to. We will call these &ldquo;identity matrices&rdquo; the <strong>coset representatives</strong>, as they represent the entire coset. An example of a coset representative that is useful to us is <em>the matrix in the coset with the smallest valued entries</em>.</p><p>Therefore, if we choose random elements of $\mathrm{SL}_2(\Z)$ and they all seem to converge to some finite set of coset representatives, we have strong evidence that there are finitely many cosets, and therefore that the subgroup $H$ has finite index in $\mathrm{SL}_2(\Z)$.</p><figure><img src=/img/machine-learning-sl2z/generators-paths-cosets.png alt="What we hope the algorithm should learn: finding a path back from any matrix to a coset representative (in blue)."><figcaption><p>What we hope the algorithm should learn: finding a path back from any matrix to a coset representative (in blue).</p></figcaption></figure><p>Here, specifically, is what we want the machine to answer:</p><blockquote><p>Given a matrix $A$, which generator should it be multiplied by to get closer to the coset representative?</p></blockquote><h3 id=our-hope-as-mathematicians>Our hope (as mathematicians)</h3><p>What we want out of the machine is <em>not</em> some algorithm that works for the matrices we feed into it. <em>Strong numerical data is not proof</em>. What we really want is an algorithm that we can prove shows the <em>finite</em>-ness of the subgroup index. With that in mind, here are our results.</p><h2 id=what-did-the-machine-learn>What did the machine learn?</h2><p>Our machine learning model took the first column of the matrix as input and would output one of the four generators that, when multiplied to the matrix, gets us closer to the identity.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> We trained a feedforward neural network with $3$ hidden layers (with $128$, $64$, and $16$ nodes), sigmoid and softmax <a href=https://keras.io/api/layers/activations/>activation functions</a> on data on paths from matrices to the coset representative in the coset $H$. We used the <a href=https://keras.io/api/optimizers/adam/>Adam</a> optimizer with a learning rate of $0.003$ and the <a href=https://keras.io/api/losses/probabilistic_losses/><code>categorical_crossentropy</code></a> loss function.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><p>Here are the resulting decision boundaries our machine learned:</p><figure><img src=/img/machine-learning-sl2z/decision-boundaries.png></figure><p>Each color corresponds to a generator. For example, when we use the matrix $\begin{bmatrix}
13 & -3 \\ -30 & 7
\end{bmatrix}$, the model sees the vector $\begin{bmatrix}
13 \\ -30
\end{bmatrix}$, and it applies $\begin{bmatrix}
1 & 0 \\ 2 & 1
\end{bmatrix}$ to get</p><p>$$
\begin{bmatrix}
13 & -3 \\ -30 & 7
\end{bmatrix} \begin{bmatrix}
1 & 0 \\ 2 & 1
\end{bmatrix} = \begin{bmatrix}
13 & -3 \\ -4 & 1
\end{bmatrix}.
$$</p><p>Notice that this reduces the size of the entries in the matrix, as we expect the model to learn.</p><h3 id=extracting-ideas-from-the-model>Extracting ideas from the model</h3><p>One thing about these decision boundaries is that they are nearly linear. Since our activation functions were not linear, the model did not create exact linear decision boundaries. However, we as humans can infer that the model is trying to make the decision boundaries on the $x$, $y$-axes and the lines $y=x$ and $y=-x$.</p><figure><img src=/img/machine-learning-sl2z/human-decision-boundaries.png></figure><p>So the machine seems to be learning this algorithm. What&rsquo;s the significance of this? Well, it is <em>exactly</em> the textbook approach for proving that the Sanov subgroup has finite index!<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> So, the machine approximately created a <strong>provably</strong> correct algorithm that shows the Sanov subgroup has finite index. So, to some effect, the machine was starting to learn how to prove something about a group!</p><h3 id=a-caveat>A caveat</h3><p>The model was doing a great job at <em>nearly</em> getting a linear decision boundary, which may make it seem like most matrices will get back to a coset representative like in the algorithm for the written proof.</p><p>However, in practice, we found that most matrices did <em>not</em> find their way back to the representative when using the machine learning algorithm; small inaccuracies in the model led to wrong decisions &ldquo;blowing up&rdquo; and preventing us from reaching the coset representatives (thanks Jordan for reminding me of this!).</p><p>Whether we can get around this obstacle is still a question to work on. Perhaps working with a model that creates linear decision boundaries would have panned out better for this specific subgroup, but we would have to see how it extends to other subgroups as well.</p><h2 id=potential-for-mathematics>Potential for mathematics</h2><p>I will admit that the last step - using human inference to get the decision boundaries - is at odds with to the dream of AI fully developing a mathematical proof. Moreover, the algorithm itself is obfuscated by the matrix multiplication and turns out to be quite simple: <em>take the vector and use twice the smaller value to decrease the absolute value of the larger value</em>. Given the right cost function, this would be easy for a machine to learn. However, learning this algorithm revealed to me that there is some potential of a machine understanding the algebraic structure of the modular group, which was far more than I expected coming into this research. Hopefully, we can apply this approach to subgroups with unknown index!</p><p>Overall, this project has opened my mind to using machine learning techniques to verify mathematical problems. The people at DeepMind have shown <a href=https://deepmind.google/discover/blog/discovering-novel-algorithms-with-alphatensor/>several</a> <a href=https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/>examples</a> of using AI models to make entirely new discoveries in math, using incredibly novel methods in artificial intelligence to do so. In contrast, I think our project prototyped how simple machine learning models could assist us in math research.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>For an introduction to modular forms, I would recommend <a href=https://ctnt-summer.math.uconn.edu/wp-content/uploads/sites/1632/2016/02/CTNTmodularforms.pdf>Keith Conrad&rsquo;s notes</a>, which were used for <a href="https://www.youtube.com/playlist?list=PLJUSzeW191Qx_rdAS8sd4nTNlSyLt97Q4">this lecture series</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>I glossed over the details because it did not concern our project. The precise definition is <a href=https://en.wikipedia.org/wiki/Holomorphic_function>holomorphic functions</a> $f \colon \mathbb{H} \to \mathbb{C}$ such that there exists a $k \in \Z$ so for all $z$,</p><p>$$
f \left( \frac{az+b}{cz+d}\right) = (cz+d)^k f(z),
$$</p><p>and that $f(z)$ is finite as $z \to \infty$.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://www.ams.org/journals/notices/201906/rnoti-p905.pdf>https://www.ams.org/journals/notices/201906/rnoti-p905.pdf</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>We did this to get a visualization that did not require $4$ dimensions. The condition that the determinant is $1$ actually means that this reduction doesn&rsquo;t lose much data for most matrices.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>We also worked on more direct algorithms, such as breadth-first search, and other machine learning models, such as <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> and <a href=https://en.wikipedia.org/wiki/Monte_Carlo_tree_search>Monte Carlo Tree Search</a>. However, we had the best results with standard neural networks.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>See Löh&rsquo;s <em>Geometric Group Theory: An Introduction</em> (Proposition 4.4.2), for example.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/machine-learning>machine learning</a></li><li><a href=/tags/algebra>algebra</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/pr4-kp/ rel=me title=GitHub><i data-feather=github></i></a></div><div class=footer-info>| 2025 © Pramana | <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script>feather.replace()</script></div></body></html>